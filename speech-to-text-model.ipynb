{"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7264012,"sourceType":"datasetVersion","datasetId":4210175},{"sourceId":7264021,"sourceType":"datasetVersion","datasetId":4210182}],"dockerImageVersionId":30627,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"! pip install jiwer","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PIUxbz-SHJQP","outputId":"2fc22da7-7977-4b42-f9d3-63cd9b3c1843","execution":{"iopub.status.busy":"2024-03-23T12:43:01.212828Z","iopub.execute_input":"2024-03-23T12:43:01.213297Z","iopub.status.idle":"2024-03-23T12:43:15.301003Z","shell.execute_reply.started":"2024-03-23T12:43:01.213256Z","shell.execute_reply":"2024-03-23T12:43:15.299862Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting jiwer\n  Obtaining dependency information for jiwer from https://files.pythonhosted.org/packages/0d/4f/ee537ab20144811dd99321735ff92ef2b3a3230b77ed7454bed4c44d21fc/jiwer-3.0.3-py3-none-any.whl.metadata\n  Downloading jiwer-3.0.3-py3-none-any.whl.metadata (2.6 kB)\nRequirement already satisfied: click<9.0.0,>=8.1.3 in /opt/conda/lib/python3.10/site-packages (from jiwer) (8.1.7)\nRequirement already satisfied: rapidfuzz<4,>=3 in /opt/conda/lib/python3.10/site-packages (from jiwer) (3.5.2)\nDownloading jiwer-3.0.3-py3-none-any.whl (21 kB)\nInstalling collected packages: jiwer\nSuccessfully installed jiwer-3.0.3\n","output_type":"stream"}]},{"cell_type":"code","source":"! pip install --upgrade pip","metadata":{"execution":{"iopub.status.busy":"2024-03-23T12:43:15.303310Z","iopub.execute_input":"2024-03-23T12:43:15.303746Z","iopub.status.idle":"2024-03-23T12:43:39.219439Z","shell.execute_reply.started":"2024-03-23T12:43:15.303700Z","shell.execute_reply":"2024-03-23T12:43:39.218275Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Requirement already satisfied: pip in /opt/conda/lib/python3.10/site-packages (23.2.1)\nCollecting pip\n  Obtaining dependency information for pip from https://files.pythonhosted.org/packages/8a/6a/19e9fe04fca059ccf770861c7d5721ab4c2aebc539889e97c7977528a53b/pip-24.0-py3-none-any.whl.metadata\n  Downloading pip-24.0-py3-none-any.whl.metadata (3.6 kB)\nDownloading pip-24.0-py3-none-any.whl (2.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m51.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: pip\n  Attempting uninstall: pip\n    Found existing installation: pip 23.2.1\n    Uninstalling pip-23.2.1:\n      Successfully uninstalled pip-23.2.1\nSuccessfully installed pip-24.0\n","output_type":"stream"}]},{"cell_type":"code","source":"! pip install pandas numpy tensorflow Ipython","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"K_vWAX6GHUAp","outputId":"5f199ee7-ffe4-4933-ca8f-bc9c61a76d81","execution":{"iopub.status.busy":"2024-03-23T12:43:39.221086Z","iopub.execute_input":"2024-03-23T12:43:39.221425Z","iopub.status.idle":"2024-03-23T12:43:52.045786Z","shell.execute_reply.started":"2024-03-23T12:43:39.221390Z","shell.execute_reply":"2024-03-23T12:43:52.044501Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (2.0.3)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (1.24.3)\nRequirement already satisfied: tensorflow in /opt/conda/lib/python3.10/site-packages (2.13.0)\nRequirement already satisfied: Ipython in /opt/conda/lib/python3.10/site-packages (8.14.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas) (2023.3)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas) (2023.3)\nRequirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.4.0)\nRequirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.6.3)\nRequirement already satisfied: flatbuffers>=23.1.21 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (23.5.26)\nRequirement already satisfied: gast<=0.4.0,>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (0.4.0)\nRequirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (0.2.0)\nRequirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.51.1)\nRequirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (3.9.0)\nRequirement already satisfied: keras<2.14,>=2.13.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (2.13.1)\nRequirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (16.0.6)\nRequirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (3.3.0)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from tensorflow) (21.3)\nRequirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (3.20.3)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from tensorflow) (68.1.2)\nRequirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.16.0)\nRequirement already satisfied: tensorboard<2.14,>=2.13 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (2.13.0)\nRequirement already satisfied: tensorflow-estimator<2.14,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (2.13.0)\nRequirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (2.3.0)\nRequirement already satisfied: typing-extensions<4.6.0,>=3.6.6 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (4.5.0)\nRequirement already satisfied: wrapt>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.15.0)\nRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (0.34.0)\nRequirement already satisfied: backcall in /opt/conda/lib/python3.10/site-packages (from Ipython) (0.2.0)\nRequirement already satisfied: decorator in /opt/conda/lib/python3.10/site-packages (from Ipython) (5.1.1)\nRequirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.10/site-packages (from Ipython) (0.19.0)\nRequirement already satisfied: matplotlib-inline in /opt/conda/lib/python3.10/site-packages (from Ipython) (0.1.6)\nRequirement already satisfied: pickleshare in /opt/conda/lib/python3.10/site-packages (from Ipython) (0.7.5)\nRequirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in /opt/conda/lib/python3.10/site-packages (from Ipython) (3.0.39)\nRequirement already satisfied: pygments>=2.4.0 in /opt/conda/lib/python3.10/site-packages (from Ipython) (2.16.1)\nRequirement already satisfied: stack-data in /opt/conda/lib/python3.10/site-packages (from Ipython) (0.6.2)\nRequirement already satisfied: traitlets>=5 in /opt/conda/lib/python3.10/site-packages (from Ipython) (5.9.0)\nRequirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.10/site-packages (from Ipython) (4.8.0)\nRequirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow) (0.41.2)\nRequirement already satisfied: parso<0.9.0,>=0.8.3 in /opt/conda/lib/python3.10/site-packages (from jedi>=0.16->Ipython) (0.8.3)\nRequirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.10/site-packages (from pexpect>4.3->Ipython) (0.7.0)\nRequirement already satisfied: wcwidth in /opt/conda/lib/python3.10/site-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->Ipython) (0.2.6)\nRequirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow) (2.22.0)\nRequirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow) (1.0.0)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow) (3.4.4)\nRequirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow) (2.31.0)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow) (0.7.1)\nRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow) (3.0.1)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->tensorflow) (3.0.9)\nRequirement already satisfied: executing>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from stack-data->Ipython) (1.2.0)\nRequirement already satisfied: asttokens>=2.1.0 in /opt/conda/lib/python3.10/site-packages (from stack-data->Ipython) (2.2.1)\nRequirement already satisfied: pure-eval in /opt/conda/lib/python3.10/site-packages (from stack-data->Ipython) (0.2.2)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (4.2.4)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (0.2.7)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (4.9)\nRequirement already satisfied: urllib3<2.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (1.26.15)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow) (1.3.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (3.2.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (3.4)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (2023.11.17)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.14,>=2.13->tensorflow) (2.1.3)\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (0.4.8)\nRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow) (3.2.2)\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport matplotlib.pyplot as plt\nfrom IPython import display\nfrom jiwer import wer","metadata":{"id":"1ecVYLnTHkoe","execution":{"iopub.status.busy":"2024-03-23T12:43:52.049012Z","iopub.execute_input":"2024-03-23T12:43:52.050017Z","iopub.status.idle":"2024-03-23T12:44:04.059826Z","shell.execute_reply.started":"2024-03-23T12:43:52.049948Z","shell.execute_reply":"2024-03-23T12:44:04.058930Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"code","source":"data_url = \"https://data.keithito.com/data/speech/LJSpeech-1.1.tar.bz2\"\ndata_path = keras.utils.get_file(\"LJSpeech-1.1\", data_url, untar=True)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NSeJPGlRICVl","outputId":"a543c8a7-13aa-46e7-ac2f-4598e78062c6","execution":{"iopub.status.busy":"2024-03-23T12:44:04.061282Z","iopub.execute_input":"2024-03-23T12:44:04.062094Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Downloading data from https://data.keithito.com/data/speech/LJSpeech-1.1.tar.bz2\n2748572632/2748572632 [==============================] - 139s 0us/step\n","output_type":"stream"}]},{"cell_type":"code","source":"wavs_path = data_path + \"/wavs/\"\nmetadata_path = data_path + \"/metadata.csv\"","metadata":{"id":"6tZx-HKUIp0l","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Read metadata file and parse it\nmetadata_df = pd.read_csv(metadata_path, sep=\"|\", header=None, quoting=3)","metadata":{"id":"eQ_pHbpwJ93T","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"metadata_df.head(10)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":363},"id":"_UPUrkfrKGeS","outputId":"463af616-4adb-413e-92b7-f19ba41c54a9","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"metadata_df.columns = [\"file_name\", \"transcription\", \"normalized_transcription\"]\nmetadata_df = metadata_df[[\"file_name\", \"normalized_transcription\"]]\nmetadata_df = metadata_df.sample (frac=1).reset_index(drop=True)\nmetadata_df.head(3)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":143},"id":"yLQXamNlKSN-","outputId":"c7d1f5db-5b3c-4a5b-df0c-ac9b0c0efd03","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#splitl=int (len (metadata_df) * 0.30)\nsplit = int(len(metadata_df) * 0.90)\ndf_train = metadata_df[:split]\ndf_val = metadata_df[split:]\nprint (f\"Size of the training set: {len (df_train)}\")\nprint (f\"Size of the training set: {len (df_val)}\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MStY3smwKxEY","outputId":"6f9a8df3-a527-4004-a995-a3c2b8d9a963","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# The set of characters accepted in the transcription.\ncharacters = [x for x in \"abcdefghijklmnopqrstuvwxyz'?! \"]\n# Mapping characters to integers\nchar_to_num = keras.layers.StringLookup(vocabulary=characters, oov_token=\"\")\n# Mapping integers back to original characters\nnum_to_char = keras.layers.StringLookup (\n  vocabulary = char_to_num.get_vocabulary(), oov_token=\"\", invert=True\n)\nprint(\n  f\"The vocabulary is: {char_to_num.get_vocabulary()}\"\n  f\"(size ={char_to_num.vocabulary_size()})\"\n)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"65V9Wq1CLKT-","outputId":"c02c85ae-d7bc-4c75-b90a-97c103941150","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"char_to_num","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Uh6VuXUwMCkf","outputId":"91e4d3eb-a506-42f5-ea08-3888f2f9e6f3","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# An integer scalar Tensor. The window length in samples.\nframe_length = 256\n# An integer scalar Tensor. The number of samples to step.\nframe_step = 160\n# An integer scalar Tensor. The size of the FFT to apply.\n# If not provided, uses the smallest power of 2 enclosing frame_length.\nfft_length = 384\n\ndef encode_single_sample(wav_file, label):\n  ## Process the Audio\n  ############******************************\n  # 1. Read wav file\n  file = tf.io.read_file(wavs_path + wav_file + \".wav\")\n  # 2. Decode the wav file\n  audio,_ = tf.audio.decode_wav(file)\n  audio = tf.squeeze(audio, axis=-1)\n  # 3. Change type to float\n  audio = tf.cast(audio, tf.float32)\n  # 4. Get the spectrogram\n  spectrogram = tf.signal.stft(\n    audio, frame_length=frame_length, frame_step=frame_step, fft_length=fft_length\n  )\n  # 5. We only need the magnitude, which can be derived by applying tf.abs\n  spectrogram = tf.abs(spectrogram)\n  spectrogram = tf.math.pow(spectrogram, 0.5)\n  # 6. normalisation\n  means = tf.math.reduce_mean(spectrogram, 1, keepdims=True)\n  stddevs = tf.math.reduce_std(spectrogram, 1, keepdims=True)\n  spectrogram = (spectrogram - means)/(stddevs + 1e-10)\n  ###########################################\n  ## Process the label\n  ############################ *###########\n  # 7. Convert label to Lower case\n  label = tf.strings.lower(label)\n  # 8. Split the label\n  label = tf.strings.unicode_split(label, input_encoding=\"UTF-8\")\n  # 9. Map the characters in label to numbers\n  label = char_to_num(label)\n  # 10. Return a dict as our model is expecting two inputs\n  return spectrogram, label","metadata":{"id":"mvhaX9TpOp7J","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Creating Dataset Objects**","metadata":{"id":"GNyUGbGPUFAZ"}},{"cell_type":"code","source":"batch_size = 32\n# Define the trainig dataset\ntrain_dataset = tf.data.Dataset.from_tensor_slices(\n  (list(df_train[\"file_name\"]), list(df_train[\"normalized_transcription\"]))\n)\ntrain_dataset = (\n  train_dataset.map(encode_single_sample, num_parallel_calls = tf.data.AUTOTUNE)\n  .padded_batch(batch_size)\n  .prefetch(buffer_size = tf.data.AUTOTUNE)\n )\n# Define the validation dataset\nvalidation_dataset = tf.data. Dataset.from_tensor_slices (\n  (list(df_val[\"file_name\"]), list (df_val[\"normalized_transcription\"]))\n)\nvalidation_dataset = (\n  validation_dataset.map(encode_single_sample, num_parallel_calls = tf.data.AUTOTUNE)\n  .padded_batch(batch_size)\n  .prefetch(buffer_size = tf.data.AUTOTUNE)\n)","metadata":{"id":"0LGW-4zfOti8","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Visualize**","metadata":{"id":"sKLkfhGYVLSd"}},{"cell_type":"code","source":"fig = plt.figure(figsize=(8,5))\nfor batch in train_dataset.take (1):\n  spectrogram = batch[0][0].numpy()\n  spectrogram = np.array([np.trim_zeros(x) for x in np.transpose(spectrogram)])\n  label = batch[1][0]\n  # Spectrogram\n  label = tf.strings.reduce_join(num_to_char(label)).numpy().decode(\"utf-8\")\n  ax = plt.subplot(2, 1, 1)\n  ax.imshow(spectrogram, vmax=1)\n  ax.set_title(label)\n  ax.axis(\"off\")\n  # Wav\n  file = tf.io.read_file(wavs_path + list(df_train[\"file_name\"])[0] + \".wav\")\n  audio,_ = tf.audio.decode_wav(file)\n  audio = audio.numpy()\n  ax = plt.subplot(2, 1, 2)\n  plt.plot(audio)\n  ax.set_title(\"Signal Wave\")\n  ax.set_xlim(0, len(audio))\n  display.display(display.Audio(np.transpose (audio), rate = 16000))\nplt.show()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":505},"id":"4L1nfbvOU9vB","outputId":"6f0873f0-de10-4090-a3aa-a08f88ad266c","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Model**","metadata":{"id":"_jKvxBUgWdGw"}},{"cell_type":"code","source":"def CTCLoss(y_true, y_pred):\n  # Compute the training-time loss value\n  batch_len = tf.cast(tf.shape(y_true)[0], dtype=\"int64\")\n  input_length = tf.cast(tf.shape(y_pred)[1], dtype=\"int64\")\n  label_length = tf.cast(tf.shape(y_true)[1], dtype=\"int64\")\n\n  input_length = input_length * tf.ones(shape= (batch_len, 1), dtype=\"int64\")\n  label_length = label_length * tf.ones(shape= (batch_len, 1), dtype=\"int64\")\n\n  loss = keras.backend.ctc_batch_cost(y_true, y_pred, input_length, label_length)\n  return loss","metadata":{"id":"rkZ995c9VxF8","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def build_model(input_dim, output_dim, rnn_layers=5, rnn_units=128):\n  \"\"\"Model similar to DeepSpeech2.\"\"\"\n  # Model's input\n  input_spectrogram = layers.Input((None, input_dim), name=\"input\")\n  # Expand the dimension to use 2D CNN.\n  x = layers.Reshape((-1, input_dim, 1), name=\"expand_dim\") (input_spectrogram)\n  # Convolution layer 1\n  x = layers.Conv2D(\n    filters=32,\n    kernel_size=[11, 41],\n    strides=[2, 2],\n    padding=\"same\",\n    use_bias=False,\n    name=\"conv_1\",\n  )(x)\n  x = layers.BatchNormalization(name=\"conv_1_bn\")(x)\n  x = layers.ReLU(name=\"conv_1_relu\")(x)\n  # Convolution layer 2\n  x = layers.Conv2D(\n    filters=32,\n    kernel_size=[11, 21],\n    strides=[1, 2],\n    padding=\"same\",\n    use_bias=False,\n    name=\"conv_2\",\n  )(x)\n  x = layers.BatchNormalization(name=\"conv_2_bn\")(x)\n  x = layers.ReLU(name=\"conv_2_relu\")(x)\n  # Reshape the resulted volume to feed the RNNS layers\n  x = layers.Reshape((-1, x.shape [-2]* x.shape [-1]))(x)\n  # RNN layers\n  for i in range(1, rnn_layers + 1):\n    recurrent = layers.GRU(\n      units = rnn_units,\n      activation  =\"tanh\",\n      recurrent_activation=\"sigmoid\",\n      use_bias=True,\n      return_sequences=True,\n      reset_after=True,\n      name=f\"gru_{i}\",\n    )\n    x = layers.Bidirectional(\n      recurrent, name=f\"bidirectional_{i}\", merge_mode=\"concat\"\n    )(x)\n    if i< rnn_layers:\n      x = layers.Dropout(rate=0.5)(x)\n\n  # Dense layer\n  x = layers.Dense(units=rnn_units * 2, name=\"dense_1\")(x)\n  x = layers.ReLU(name=\"dense_1_relu\")(x)\n  x = layers.Dropout(rate=0.5)(x)\n  # Classification layer\n  output = layers.Dense(units = output_dim + 1, activation=\"softmax\")(x)\n  # Model\n  model = keras.Model(input_spectrogram, output, name=\"DeepSpeech_2\")\n  # Optimizer\n  opt = keras.optimizers.Adam(learning_rate=1e-4)\n  # Compile the model and return\n  model.compile(optimizer=opt, loss=CTCLoss)\n  return model\n\n# Get the model\nmodel = build_model(\n  input_dim = fft_length // 2 + 1,\n  output_dim = char_to_num.vocabulary_size(),\n  rnn_units = 512,\n)\nmodel.summary(line_length = 110)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0NVkjzR2XCj8","outputId":"a0cc588d-b1fe-4d4d-8c51-c8d2d537e4a0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Training and evaluation**","metadata":{"id":"j1H2NKJIaCr9"}},{"cell_type":"code","source":"# A utility function to decode the output of the network\ndef decode_batch_predictions(pred):\n  input_len = np.ones(pred. shape[0]) * pred.shape[1]\n  # Use greedy search. For complex tasks, you can use beam search\n  results = keras.backend.ctc_decode(pred, input_length=input_len, greedy=True)[0][0]\n  # Iterate over the results and get back the text\n  output_text= []\n  for result in results:\n    result = tf.strings.reduce_join(num_to_char(result)).numpy().decode(\"utf-8\")\n    output_text.append(result)\n  return output_text\n\n# A callback class to output a few transcriptions during training\nclass CallbackEval(keras.callbacks.Callback):\n  \"\"\"Displays a batch of outputs after every epoch.\"\"\"\n\n  def __init__(self, dataset):\n    super().__init__()\n    self.dataset = dataset\n\n  def on_epoch_end(self, epoch: int, logs=None):\n    predictions = []\n    targets = []\n    for batch in self.dataset:\n      X, y = batch\n      batch_predictions = model.predict(X)\n      batch_predictions = decode_batch_predictions(batch_predictions)\n      predictions.extend(batch_predictions)\n      for label in y:\n        label = (\n        tf.strings.reduce_join(num_to_char(label)).numpy().decode(\"utf-8\")\n        )\n        targets.append(label)\n    wer_score = wer(targets, predictions)\n    print(\"-\" * 100)\n    print(f\"Word Error Rate: {wer_score:.4f}\")\n    print(\"-\" * 100)\n\n    for i in np.random.randint(0,len(predictions),2):\n      print(f\"Target     : {targets[i]}\")\n      print(f\"Predictions: {predictions[i]}\")\n      print(\"-\" * 100)","metadata":{"id":"h7DSNWyNZjeE","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Train the model**","metadata":{"id":"mqQdS7L8cKf-"}},{"cell_type":"code","source":"# Define No of epochs\nepochs = 55\n# callback function to check transcription on the val set\nvalidations_callback = CallbackEval(validation_dataset)\n# Train the model\nhistory = model.fit(\n    train_dataset,\n    validation_data = validation_dataset,\n    epochs = epochs,\n    callbacks = [validations_callback]\n)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0YbNZU2ya9id","outputId":"deeab1a4-086a-4ae2-9e5d-ddcc310e9613","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Inference**","metadata":{"id":"CEFTKrJRdOVo"}},{"cell_type":"code","source":"# Convert the model to TensorFlow Lite format\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\nconverter._experimental_lower_tensor_list_ops = False  # Disable tensor list ops\ntflite_model = converter.convert()\n\n# Save the TensorFlow Lite model to a file\nwith open(\"speech_to_text_model.tflite\", \"wb\") as f:\n    f.write(tflite_model)\n\nprint(\"TensorFlow Lite model saved successfully.\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions = []\ntargets = []\nfor batch in validation_dataset:\n  X,y = batch\n  batch_predictions = model.predict(X)\n  batch_predictions = decode_batch_predictions(batch_predictions)\n  predictions.extend(batch_predictions)\n  for label in y:\n    label = tf.strings.reduce_join(num_to_char(label)).numpy().decode(\"utf-8\")\n    targets.append(label)\nwer_score = wer(targets, predictions)\nprint(\"-\" * 100)\nprint(f\"Word Error Rate: {wer_score:.4f}\")\nprint(\"-\" * 100)\nfor i in np.random.randint(0,len(predictions),5):\n    print(f\"Target     : {targets[i]}\")\n    print(f\"Predictions: {predictions[i]}\")\n    print(\"-\" * 100)","metadata":{"id":"oLKo3nwGcwnV","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\n\n# Load your Keras model\nmodel = tf.keras.models.load_model('/kaggle/input/stt-keras/speech_to_text_model_60.keras')  # Replace 'your_model.h5' with the actual path to your .h5 model file\n\n# Convert the Keras model to TensorFlow Lite format\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\ntflite_model = converter.convert()\n\n# Save the TensorFlow Lite model to a file\nwith open('speech_to_text_model 60 epochs.tflite', 'wb') as f:\n    f.write(tflite_model)\n\nprint(\"TensorFlow Lite model saved successfully.\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}